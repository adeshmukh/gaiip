{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nF3S6S2F926b"
      ],
      "authorship_tag": "ABX9TyN1HAGPzqxZWFwKvvfls+Bh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeshmukh/gaiip/blob/main/notebooks/Intro_to_Observability_LangSmith_Phoenix_LangFuse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Observability of Gen AI applications\n",
        "\n",
        "This notebook introduces the notion of observability for Gen AI appliations that use the LangChain framework. The use of popular observability frameworks like LangSmith, Arize Phoenix, and LangFuse is illustrated in this notebook."
      ],
      "metadata": {
        "id": "hZ6m2Z4C_N4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "U88rJAVH9bk3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pmxhrxz6wRH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install langchain              # Base LangChain\n",
        "!pip install langchain-core         # Core LangChain components\n",
        "!pip install langchain-community    # Extenteded list of LangChain components\n",
        "!pip install langchain-openai       # LangChain bindings for OpenAI LLM\n",
        "!pip install langchain-chroma       # LangChain bindings for ChromaDB\n",
        "!pip install langchainhub           # Hub for prompt templates\n",
        "\n",
        "!pip install langsmith              # LangSmith for observability\n",
        "!pip install arize-phoenix          # Phoenix for observability\n",
        "!pip install langfuse               # LangFuse for observability\n",
        "\n",
        "!pip install openai                 # OpenAI LLM provider\n",
        "!pip install google-search-results  # Serp API web search provider\n",
        "!pip install pypdf                  # Parse PDF docs\n",
        "!pip install chromadb               # Vector DB\n",
        "\n",
        "!pip install tiktoken               # Tokenizer for OpenAI LLM\n",
        "!pip install nest-asyncio           # Async IO in notebook environments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize secrets as environment variables."
      ],
      "metadata": {
        "id": "v9JCsDYRw4dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# API keys we will need for this demo.\n",
        "api_key_names = [\n",
        "    'LANGCHAIN_API_KEY',\n",
        "    'OPENAI_API_KEY',\n",
        "    'SERPAPI_API_KEY',\n",
        "    'LANGFUSE_SECRET_KEY',  # Optional, only needed for the LangFuse portion.\n",
        "  ]\n",
        "\n",
        "for api_key in api_key_names:\n",
        "  os.environ[api_key]=userdata.get(api_key)"
      ],
      "metadata": {
        "id": "6gAmKY156-cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. LangSmith"
      ],
      "metadata": {
        "id": "dDKHRJ7U9etm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, enable tracing and provide the tracing endpoint."
      ],
      "metadata": {
        "id": "sE4wLt9I_A_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'"
      ],
      "metadata": {
        "id": "wurjcj5q7eVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic instrumentation\n"
      ],
      "metadata": {
        "id": "u1DLKSWC9p9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are using LangChain components without explicitly using any langsmith functions/decorators. This example illustrates that standard LangChain components are automatically observable with LangSmith once the environment variables are configured appropriately."
      ],
      "metadata": {
        "id": "AFJ5jqTg-fZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "llm.invoke(\"Hello, world!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xeem8SpE8cgA",
        "outputId": "65db12ae-30a2-48d3-98a8-8e9c17a00ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello there! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ddcfbefa-94e1-49d4-ba8c-48ab351bacbf-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the above code cell, check the LangSmith dashboard for a corresponding trace.\n",
        "\n",
        "*NOTE: If you don't see a trace in LangSmith, then something's not right with your setup. Pause here and fix any issues before proceeding with the rest of the notebook.*"
      ],
      "metadata": {
        "id": "fH61BJmbjLKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instrumenting with LangSmith SDK & Decorator\n",
        "\n",
        "Tracing SDK enables wrapping non-langchain components to make them observable.\n",
        "\n",
        "Tracing decorator enables tracing for any application method."
      ],
      "metadata": {
        "id": "nF3S6S2F926b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langsmith.wrappers import wrap_openai\n",
        "from langsmith import traceable\n",
        "\n",
        "# Auto-trace LLM calls in-context\n",
        "client = wrap_openai(openai.Client())\n",
        "\n",
        "@traceable # Auto-trace this function\n",
        "def llm_chat(user_input: str):\n",
        "    result = client.chat.completions.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
        "        model=\"gpt-3.5-turbo\"\n",
        "    )\n",
        "    return result.choices[0].message.content\n",
        "\n",
        "countries = ['United States', 'Denmark', 'Egypt', 'China', 'India', 'Vietnam', 'Australia']\n",
        "\n",
        "for country in countries:\n",
        "  print(llm_chat(f'What is the capital of {country}?'))"
      ],
      "metadata": {
        "id": "idBmTM687jN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b94f86-a2f1-4537-f1c2-c9d2fc42212f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of the United States is Washington, D.C.\n",
            "Copenhagen\n",
            "The capital of Egypt is Cairo.\n",
            "The capital of China is Beijing.\n",
            "The capital of India is New Delhi.\n",
            "The capital of Vietnam is Hanoi.\n",
            "The capital of Australia is Canberra.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, check the LangSmith dashboard and observe the traces generated for each of the calls to `llm_chat` in the loop. So, in total you should observe `7` calls corresponding to the `7` items in the countries list above."
      ],
      "metadata": {
        "id": "VuvuNX6Oje7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Q: Was the output from the LLM what you expected? If not, you can try adjusting the prompt or experiment with the prompt here and observe the LangSmith dashboard.*"
      ],
      "metadata": {
        "id": "pabHoT1Ijz_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReAct Agent\n",
        "\n",
        "A ReAct agent operates in a Think-Observe-Action loop until it achieves it's goal of getting a satisfactory answer to the original question or until it exhausts maximum number of tries.\n",
        "\n",
        "We will construct a simply ReAct agent that uses 3 tools:\n",
        "1. **Retriever**: To search specific documents that we indexed in an in-memory Vector DB.\n",
        "2. **Search**: To search current information on the web.\n",
        "3. **Calculator**: To evaluate math expressions."
      ],
      "metadata": {
        "id": "oz-BDX4V650F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize Vector DB used for the Retriever tool\n",
        "\n",
        "For this demo, we will fetch PDF files that contain information pertaining to the US National Budget for the fiscal year 2024. This will be loaded into a Vector DB (Chroma) and made available to the agent as a LangChain tool."
      ],
      "metadata": {
        "id": "JXkJjWRR7l77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will clone the repo that contains the PDFs."
      ],
      "metadata": {
        "id": "t_AJDem3gmtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!rm -rf ./repo\n",
        "!git clone --depth 1 https://github.com/adeshmukh/gaiip.git ./repo"
      ],
      "metadata": {
        "id": "acHakKhy7yyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we load the PDFs as chunks of text using PyPDFLoader and store it in an in-memory list."
      ],
      "metadata": {
        "id": "JxTGrAXYhpCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# Ignore pypdf warnings\n",
        "logging.getLogger('pypdf').setLevel(logging.ERROR)\n",
        "\n",
        "base_path = './repo/pdfs'\n",
        "pdf_documents = os.listdir(base_path)\n",
        "docs = []\n",
        "\n",
        "# Load docs as Chroma Document objects\n",
        "for pdf_document in pdf_documents:\n",
        "    pdf_loader = PyPDFLoader(f'{base_path}/{pdf_document}')\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "    pages = pdf_loader.load_and_split(text_splitter=splitter)\n",
        "    docs.extend(pages)\n",
        "    print(f'Loaded {len(pages):2} pages from {pdf_document}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCJcLZv17wqc",
        "outputId": "e4edfea9-3646-4fe9-979a-289300a81199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded  7 pages from us-national-budget-civil-fy24.pdf\n",
            "Loaded  7 pages from us-national-budget-treasury-fy24.pdf\n",
            "Loaded  7 pages from us-national-budget-nsf-fy24.pdf\n",
            "Loaded 31 pages from us-national-budget-health-fy24.pdf\n",
            "Loaded  6 pages from us-national-budget-ssa-fy24.pdf\n",
            "Loaded  6 pages from us-national-budget-nasa-fy24.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will initialize the Vector DB (Chroma) with the list of text chunks that we loaded previously.\n",
        "\n",
        "Then we will create a LangChain tool that wraps the Vector DB. The tool is an adapter that makes it easy to plug it in the ReAct agent."
      ],
      "metadata": {
        "id": "yH3xwprKh5bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')\n",
        "\n",
        "# Initialize the vector DB with the docs\n",
        "vector_db = Chroma.from_documents(docs, embedding_function)\n",
        "\n",
        "# Create a retriever adapter for this vector db\n",
        "retriever = vector_db.as_retriever(search_type='mmr')"
      ],
      "metadata": {
        "id": "AmOSIQf277SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try using the retriever as a standalone utility, i.e. without integrating it in a ReAct agent."
      ],
      "metadata": {
        "id": "rf_btWJ5iyeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try querying the retriever to see how it functions\n",
        "hits = retriever.invoke('What is the total requested budget for NASA and NSF?')\n",
        "\n",
        "for hit in hits[:2]:  # Peek at the top 3 hits\n",
        "    print('-------------------------------------------')\n",
        "    print(hit.page_content, hit.metadata, sep='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoNwJXGD7--j",
        "outputId": "df969233-ec81-4627-9a00-8fce1c4539fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "127NATIONAL SCIENCE FOUNDATIONThe National Science Foundation (NSF) is responsible for science education and promoting the \n",
            "progress of science and innovation.  The President‚Äôs 2024 Budget for NSF advances the goals \n",
            "of the CHIPS and Science Act by:  strengthening U.S. leadership in emerging technologies; \n",
            "expanding the science, technology, engineering, and mathematics (STEM) workforce while \n",
            "advancing equity; boosting research and development (R&D) including research infrastructure; \n",
            "and combating the climate crisis. \n",
            "The Budget requests $11.3 billion in discretionary budget authority for 2024, a $1.8 billion or \n",
            "18.6-percent increase from the 2023 enacted level to support the CHIPS and Science Act.\n",
            "The President‚Äôs 2024 Budget:  \n",
            "‚Ä¢ Ensures the Future Is Made in America.   NSF plays a key role in the CHIPS and Science \n",
            "Act with its focus on U.S. leadership in new technologies‚Äîfrom artiÔ¨Åcial intelligence to\n",
            "\n",
            "{'page': 0, 'source': './repo/pdfs/us-national-budget-nsf-fy24.pdf'}\n",
            "-------------------------------------------\n",
            "completion of the Artemis I mission in 2022, the Budget provides $8.1 billion, a $500 million in -\n",
            "crease above the 2023 enacted level, for the Artemis program of lunar exploration.  The Budget \n",
            "fully funds the rockets, crew vehicle, lunar landers, space suits, and other systems needed \n",
            "to Ô¨Çy astronauts around the Moon on the Artemis II mission and then land astronauts‚Äîin -\n",
            "cluding the Ô¨Årst woman, Ô¨Årst person of color, and Ô¨Årst astronauts from another nation‚Äîon \n",
            "subsequent Artemis missions on the lunar surface as America begins development of a lunar \n",
            "outpost and aims toward the eventual exploration of Mars.\n",
            "‚Ä¢ Advances Robotic Exploration of Mars.   The Budget continues U.S. leadership in Mars \n",
            "exploration by working in concert with other nations to develop Mars missions that would \n",
            "advance the search for potential life on other planets and pave the way for human explora -\n",
            "tion.  SpeciÔ¨Åcally, the Budget provides $949 million for the U.S.-led Mars Sample Return\n",
            "\n",
            "{'page': 0, 'source': './repo/pdfs/us-national-budget-nasa-fy24.pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the above cell successfully, check the LangSmith dashboard for corresponding traces."
      ],
      "metadata": {
        "id": "79m2ENooi6jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ReAct agent without Vector DB\n",
        "\n",
        "First, we create a ReAct agent with just the `search` and `llm-math` tools and observe the output."
      ],
      "metadata": {
        "id": "wYe47k5gkqBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import (\n",
        "    create_react_agent,\n",
        "    load_tools,\n",
        "    AgentType,\n",
        "    AgentExecutor,\n",
        ")\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "LLM_MODEL = 'gpt-3.5-turbo'\n",
        "\n",
        "# More OpenAI models at: https://platform.openai.com/docs/models/\n",
        "# LLM_MODEL = 'gpt-4-turbo'\n",
        "# LLM_MODEL = 'gpt-4o'\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=LLM_MODEL,\n",
        "    temperature=0,  # temperature = 0 results in fewer hallucinations\n",
        ")\n",
        "\n",
        "# Fetch the ReAct agent prompt template from LangChain hub.\n",
        "prompt_template = hub.pull('hwchase17/react')\n",
        "\n",
        "# Tools that agents can use to augment their capabilities\n",
        "tools = load_tools(['serpapi', 'llm-math'], llm=llm)"
      ],
      "metadata": {
        "id": "prexCI868SwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a quick look at the tools\n",
        "print('\\n'.join(str(t) for t in tools))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVb-QdK4zrCI",
        "outputId": "e88d8270-dd0f-4545-e071-b7bc8399ead2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Search' description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.' func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='b8eb91f1f26a99c8f90125ab13a19f076a39bc98e0ad32a1545c27d577b0f2b2', aiosession=None)> coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='b8eb91f1f26a99c8f90125ab13a19f076a39bc98e0ad32a1545c27d577b0f2b2', aiosession=None)>\n",
            "name='Calculator' description='Useful for when you need to answer questions about math.' func=<bound method Chain.run of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7a4dfc978520>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7a4dfc979f30>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')))> coroutine=<bound method Chain.arun of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7a4dfc978520>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7a4dfc979f30>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ReAct agent\n",
        "react_agent = create_react_agent(llm, tools, prompt_template)\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(agent=react_agent, tools=tools)"
      ],
      "metadata": {
        "id": "8Rf2IVFA18yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "What is the total requested budget for NASA and NSF for Fiscal Year 2024?\n",
        "Important instructions:\n",
        "1. You must not round the final answer.\n",
        "2. Answer must be human-readable but brief.\n",
        "\"\"\"\n",
        "\n",
        "response = agent_executor.invoke(\n",
        "    {'input': query},\n",
        "    # config={\"callbacks\": [ConsoleCallbackHandler()]},  # Print events internal to the ReAct agent.\n",
        ")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UGnYCI3_8dD2",
        "outputId": "45e937b2-c73e-4ffc-d8e8-12922b0f2c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total requested budget for NASA and NSF for Fiscal Year 2024 is $36.23 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ReAct agent with retriever tool\n",
        "Now we activate the retriever tool by adding it to the list of tools. Basically, we can now compare the difference in output with & without the use of the custom retriever tool."
      ],
      "metadata": {
        "id": "6SBimBijk4Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "# Wrap the retriever in a LangChain \"tool\"\n",
        "query_us_budget_fy2024 = create_retriever_tool(\n",
        "    retriever,\n",
        "    'us_fiscal_budget_fy2024',\n",
        "    'Searches and returns excerpts from the US national budget for the fiscal year 2024.',\n",
        ")"
      ],
      "metadata": {
        "id": "Fe2HoWC1ykFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if query_us_budget_fy2024 not in tools:\n",
        "  tools.insert(0, query_us_budget_fy2024)\n",
        "\n",
        "# Let's take a quick look at the tools\n",
        "print(\"\\n\".join(str(t) for t in tools))"
      ],
      "metadata": {
        "id": "yfefiZPN8kFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04656692-8f3a-46f1-fc78-db59ef0b2495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='us_fiscal_budget_fy2024' description='Searches and returns excerpts from the US national budget for the fiscal year 2024.' args_schema=<class 'langchain_core.tools.RetrieverInput'> func=functools.partial(<function _get_relevant_documents at 0x7a4e0e3975b0>, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7a4e0ddd0280>, search_type='mmr'), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n') coroutine=functools.partial(<function _aget_relevant_documents at 0x7a4e0e397760>, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7a4e0ddd0280>, search_type='mmr'), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n')\n",
            "name='Search' description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.' func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='b8eb91f1f26a99c8f90125ab13a19f076a39bc98e0ad32a1545c27d577b0f2b2', aiosession=None)> coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='b8eb91f1f26a99c8f90125ab13a19f076a39bc98e0ad32a1545c27d577b0f2b2', aiosession=None)>\n",
            "name='Calculator' description='Useful for when you need to answer questions about math.' func=<bound method Chain.run of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7a4dfc97a230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7a4df5c09150>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')))> coroutine=<bound method Chain.arun of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7a4dfc97a230>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7a4df5c09150>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to create the `agent` and `agent_executor` with this updated list of `tools`."
      ],
      "metadata": {
        "id": "Zb5qvWoPZgVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "react_agent = create_react_agent(llm, tools, prompt_template)\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(agent=react_agent, tools=tools)\n",
        "\n",
        "response = agent_executor.invoke(\n",
        "    {'input': query},\n",
        "    # config={\"callbacks\": [ConsoleCallbackHandler()]},\n",
        ")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "id": "vufdJeKYZaei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a532f9-ca32-4363-ecc6-79b5ca2a9043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total requested budget for NASA and NSF for Fiscal Year 2024 is $38.5 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Q: Are the answers the same? If not, which is the correct answer?*\n",
        "\n",
        "*Q: What does the trace say about the agent execution?*"
      ],
      "metadata": {
        "id": "vy54h3f1CkgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Phoenix\n",
        "\n",
        "Phoenix is a free, open-source observability tool from Arize.\n",
        "\n",
        "Arize also has a SaaS observability solution available as a paid option. The SaaS solution has more features and is perhaps a better fit for production systems than the open-source solution."
      ],
      "metadata": {
        "id": "GincQu2hQUJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Launch the Phoenix server."
      ],
      "metadata": {
        "id": "clZbcTp5sbFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import phoenix as px\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "session = px.launch_app()"
      ],
      "metadata": {
        "id": "eQ3_NjQ4QSsE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fdda1f64-3d99-441b-ff07-007099193e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåç To view the Phoenix app in your browser, visit https://zkwjimvu5fd3-496ff2e9c6d22116-6006-colab.googleusercontent.com/\n",
            "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instrument LangChain for Phoenix observability.\n"
      ],
      "metadata": {
        "id": "YTlZwdkyr6mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from phoenix.trace.langchain import LangChainInstrumentor\n",
        "\n",
        "LangChainInstrumentor().instrument()"
      ],
      "metadata": {
        "id": "GIzfSpQMr5ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the same agent components that were used in the previous example."
      ],
      "metadata": {
        "id": "a3erdNOMz4Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ReAct agent\n",
        "# react_agent = create_react_agent(llm, tools, prompt)\n",
        "# agent_executor = AgentExecutor.from_agent_and_tools(agent=react_agent, tools=tools)\n",
        "response = agent_executor.invoke(\n",
        "    {'input': query},\n",
        "    # config={\"callbacks\": [ConsoleCallbackHandler()]},  # Print events internal to the ReAct agent.\n",
        ")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htneLAGwtcIU",
        "outputId": "ed0cec09-b141-40c0-884b-f07a9c96850e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total requested budget for NASA and NSF for Fiscal Year 2024 is $38.5 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. LangFuse\n",
        "\n"
      ],
      "metadata": {
        "id": "NAPCocVB5Ntx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGFUSE_PUBLIC_KEY'] = 'pk-lf-66a42a10-d9b1-4e8b-94a5-21aaf0c587d3'\n",
        "os.environ['LANGFUSE_HOST'] = \"https://us.cloud.langfuse.com\""
      ],
      "metadata": {
        "id": "E-54VgrUDkrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangFuse uses the callback mechanism in LangChain to instrument the execution."
      ],
      "metadata": {
        "id": "eHzVWD3DDo1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langfuse.callback import CallbackHandler\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "response = agent_executor.invoke(\n",
        "    {'input': query},\n",
        "    config={\"callbacks\": [langfuse_handler]},\n",
        ")\n",
        "\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZWylrHv77SP",
        "outputId": "8737c6e6-2333-4390-c926-3f9c9d0bf1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The requested budget for NASA is $27.2 billion and for NSF is $11.3 billion for Fiscal Year 2024.\n"
          ]
        }
      ]
    }
  ]
}
